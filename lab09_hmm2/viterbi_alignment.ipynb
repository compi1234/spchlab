{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1-Sg0pH041mX"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/compi1234/spchlab/blob/main/lab09_hmm2/viterbi_alignment.ipynb\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Google Colab\" title=\"Open in Google Colab\"></a> \n",
    "\n",
    "# Viterbi Alignment\n",
    "\n",
    "Viterbi Alignment and Trellis based Recognition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################\n",
    "### RUNNING THIS CELL FIRST ##########\n",
    "### will suppresses warnings on memory leaks, deprecation warnings and future warnings \n",
    "### It is brute force .  \n",
    "### Best is not to run it when you want to debug code or new installations\n",
    "import os, warnings \n",
    "os.environ[\"OMP_NUM_THREADS\"] = '2'  \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "####################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install git+https://github.com/compi1234/pyspch.git\n",
    "try:\n",
    "    import pyspch\n",
    "except ModuleNotFoundError:\n",
    "    try:\n",
    "        print(\n",
    "        \"\"\"\n",
    "        To enable this notebook on platforms as Google Colab, \n",
    "        install the pyspch package and dependencies by running following code:\n",
    "\n",
    "        !pip install git+https://github.com/compi1234/pyspch.git\n",
    "        \"\"\"\n",
    "        )\n",
    "    except ModuleNotFoundError:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'x_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 40\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     38\u001b[0m     device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mx_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m plot_probs, get_test_file\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'x_utils'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import io, os, sys\n",
    "import logging\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import requests\n",
    "import importlib\n",
    "import urllib.request\n",
    "import pickle\n",
    "import gzip\n",
    "import re\n",
    "import torch\n",
    "from sklearn import metrics as skmetrics \n",
    "from IPython.display import display, HTML, Audio\n",
    "\n",
    "# print and plot\n",
    "np.set_printoptions(precision=3)\n",
    "cmap_jet2 = sns.mpl_palette(\"jet\",60)[5:55]\n",
    "\n",
    "# pyspch\n",
    "import pyspch\n",
    "import pyspch.core as Spch\n",
    "import pyspch.display as Spd\n",
    "import pyspch.stats.probdist as Densities\n",
    "import pyspch.stats.libhmm as libhmm\n",
    "from pyspch.stats import GMM, Prob\n",
    "import pyspch.nn\n",
    "\n",
    "# device\n",
    "torch.manual_seed(0)\n",
    "use_cuda_if_available = True\n",
    "if use_cuda_if_available:\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "from x_utils import plot_probs, get_test_file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Auxiliary functions \n",
    "# dictionairy\n",
    "class dotdict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "    \n",
    "def dict_from_module(module):\n",
    "    context = {}\n",
    "    for setting in dir(module):\n",
    "        # you can write your filter here\n",
    "        if not setting.startswith('_'):\n",
    "            context[setting] = getattr(module, setting)\n",
    "\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def select_phones_idx(phn_lists,phn_set,SORTED=False):\n",
    "    ''' \n",
    "    creates a unique list of phones from a set of phone lists\n",
    "    return the unique list AND their indices in a global phone set\n",
    "    the order of appearance is preserved or sorted according to phn_set\n",
    "    '''\n",
    "    if not isinstance(phn_lists,tuple):\n",
    "        phn_lists = ([],phn_lists)\n",
    "    combined_list = []\n",
    "    for phn_list in phn_lists:\n",
    "        combined_list.extend(list(phn_list))\n",
    "    select_list = []\n",
    "    for phn in combined_list:\n",
    "        if phn not in(select_list): select_list.append(phn) # list(np.unique(select_list))\n",
    "    if SORTED:\n",
    "        select_list = [ phn  for phn in phn_set if (phn in select_list) ] # sorted(select_list)\n",
    "    select_idx = [phn_set.index(phn) for phn in select_list]\n",
    "    return(select_list,select_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_graph(phn_seq,phn_set,l2r=1.0,eps=0.0,skip=0.0,style=\"VIT\"):\n",
    "    ''' \n",
    "    makes an HMM Graph to do recognition or alignment\n",
    "    A transition matrix for a phone sequence (phn_seq) is created, all belonging to the alphabet phn_set\n",
    "    Transition probabilities are mainly 0's and 1's \n",
    "    \n",
    "    - default is make a left-to-right graph of phn_seq\n",
    "        i.e. probability=1.0 on self_loop and L2R arcs\n",
    "        \n",
    "    - l2r:  probability on L2R arcs\n",
    "    - eps:  probability on all ergodic arcs \n",
    "    - skip: probability for the skip arcs\n",
    "\n",
    "    The end result needs to be normalized to make true probabilities out of it\n",
    "    '''\n",
    "    phn2idx = {lab: i for i, lab in enumerate(phn_set)}\n",
    "    n_seq = len(phn_seq)\n",
    "    obs_indx = np.zeros(n_seq,dtype='int')\n",
    "    trans_mat = np.eye(n_seq,dtype='float32')\n",
    "\n",
    "    for i  in range(n_seq): \n",
    "        obs_indx[i] = phn2idx[phn_seq[i]]\n",
    "\n",
    "    if style == \"VIT\":\n",
    "        init_mat = np.zeros(n_seq,dtype='float32')\n",
    "        init_mat[0] = 1.0\n",
    "        end_states = np.array([n_seq-1])\n",
    "        # add left-to-right arcs\n",
    "        for i in range(n_seq-1):\n",
    "            trans_mat[i,i+1] = np.maximum(l2r,trans_mat[i,i+1])\n",
    "            if i+2 < n_seq:\n",
    "                trans_mat[i,i+2] = np.maximum(eps,skip)\n",
    "                \n",
    "    elif style == \"RECO\":\n",
    "        init_mat = np.ones(n_seq,dtype='float32')\n",
    "        end_states = np.arange(n_seq)\n",
    "\n",
    "    # set eps probabilities\n",
    "    trans_mat = np.maximum(eps,trans_mat)\n",
    "    init_mat = np.maximum(eps,init_mat)\n",
    "    \n",
    "    return(obs_indx,trans_mat,init_mat,end_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the Observation Model\n",
    "\n",
    "### Feature Extraction\n",
    "MFCC26=  MFCC13 + Deltas + mean-variance normalization, (defined by feature_args).\n",
    "### GMM models\n",
    "**S41_D26_G64_FULL_1.pkl**  G64=64 Gaussians, S41= 41 classes/states, D26 = 26 dim features, (FrameRecog: 56%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task setup\n",
    "root_url = 'https://homes.esat.kuleuven.be/~spchlab/data/'\n",
    "timit_root = root_url + 'timit/'\n",
    "#\n",
    "# define feature extraction: MFCC13 + delta's + delta_delta's mean_variance_normalization\n",
    "#\n",
    "feature_args = {'spg': None,  # tells feature extraction to start from wav files\n",
    " 'Deltas': 'delta_delta2',\n",
    " 'Norm': 'meanvar',\n",
    " 'sample_rate': 16000,\n",
    " 'f_shift': 0.01,\n",
    " 'f_length': 0.03,\n",
    " 'preemp': 0.97,\n",
    " 'window': 'hamm',\n",
    " 'mode': 'dB',\n",
    " 'n_mels': 24,\n",
    " 'n_cep': 13}\n",
    "#\n",
    "# load an existing GMM model\n",
    "#\n",
    "gmm_root = timit_root+'models/gmm/'\n",
    "gmm_model = Spch.load_data(\"S41_D26_G64_FULL_1.pkl\",root=gmm_root)  # D26 = 26 dim features, G64 = 64 Gaussians, TIMIT = training database\n",
    "gmm_nftrs = 26  # only use 26 features with gmms\n",
    "phn_set= gmm_model.classes\n",
    "phn2idx = {lab: i for i, lab in enumerate(phn_set)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# read the DNN model\n",
    "#\n",
    "# = mfcc13, delta_delta2, meanvar, 11 frames, stride 2\n",
    "model_path = 'models/default/mfcc13dd2mv/N5s2/' \n",
    "#model_path = 'models/default/mfcc13dd2mv/N0s1/' \n",
    "\n",
    "# read checkpoint \n",
    "model_fobj = pyspch.read_fobj(timit_root + model_path + 'model.pt')\n",
    "checkpoint = pyspch.nn.read_checkpoint(model_fobj, device)\n",
    "\n",
    "# unpack checkpoint (model, etc)\n",
    "setup, lab2idx, model, criterion, optimizer, scheduler = checkpoint\n",
    "# readout for one-hot encoding \n",
    "idx2lab = {v: k for k, v in lab2idx.items()}\n",
    "\n",
    "setup = dotdict(setup)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an sklearn style wrapper our torch DNN models ... but with some differences\n",
    "# WARNING -- BE CAREFULL !! \n",
    "#    the DNN does splicing and expects to see a stream of feature vectors to do the splicing\n",
    "#    the calls will give good results for long sequences\n",
    "#    this is NOT intended for frame by frame input and hence will not work smoothly\n",
    "#    in cooperation with the libhmm library used below; so all observation_probs need to be computed before the trellis computations\n",
    "class DNN_model():\n",
    "    def __init__(self,priors=None,setup=None,model=None,device=\"cpu\"):\n",
    "        self.prob_style = \"Posteriors\"\n",
    "        self.class_prior_ = priors\n",
    "        self.setup = setup\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        \n",
    "    def predict_proba(self,X):\n",
    "        # X comes in as [n_samples, n_features], but splice_frames asssumes [n_features,n_samples]\n",
    "        X_dnn = pyspch.sp.splice_frames(X.T,self.setup.sampler_args['N'], self.setup.sampler_args['stride'])\n",
    "        # posteriors \n",
    "        X_t = torch.tensor(X_dnn).T.float().to(self.device)\n",
    "        yp_t = self.model(X_t) # log probs\n",
    "        yp_t = torch.nn.Softmax(dim=1)(yp_t) # probs\n",
    "        proba = yp_t.cpu().detach().numpy()\n",
    "        return(proba)\n",
    "        \n",
    "    def predict_prob(self,X):\n",
    "        proba = self.predict_proba(X)\n",
    "        if self.class_prior_ is None: probs = proba\n",
    "        else:\n",
    "            probs = proba / self.class_prior_\n",
    "            probs = probs / np.sum(probs,axis=1,keepdims=True)\n",
    "        return ( probs )\n",
    "        \n",
    "    def predict_log_prob(self,X):\n",
    "        Xp = self.predict_prob(X)\n",
    "        return ( Spch.logf( Xp ) ) \n",
    "\n",
    "def proba_2_prob(proba,priors=None):\n",
    "    ''' compute normalized likelihoods (sum = 1) from posterior probabilities\n",
    "    '''\n",
    "    if priors is None: return(proba)\n",
    "    else:\n",
    "        probs = proba / priors\n",
    "        probs = probs / np.sum(probs,axis=1,keepdims=True)\n",
    "    return ( probs )\n",
    "\n",
    "# the TIMIT phone class priors have been stored in the GMM model\n",
    "# and be used with the DNN to convert from posteriors to (normalized) likelihoods \n",
    "dnn_model = DNN_model(setup=setup, model=model, priors= gmm_model.class_prior_, device=device)    \n",
    "dnn_nftrs = 39"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Run Example File\n",
    "\n",
    "The following cells take care of\n",
    "1. loading test sample and segmentations\n",
    "2. spg and feature computation\n",
    "3. set some focus time to highlight one or a couple of words in a full sentence\n",
    "\n",
    "The first cell will load our prototypical \"friendly computers\"; the second cell loads one of a few preselected TIMIT test files.\n",
    "Just execute one of these cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading a test file (non-TIMIT)\n",
    "file_id =  'demo/friendly'\n",
    "focus_times = np.array([0.48,1.04])\n",
    "type = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading a test file (non-TIMIT)\n",
    "file_id =  'misc/bad_bead_booed'\n",
    "focus_times = np.array([1.5,2.1])\n",
    "type = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading a test file (non-TIMIT)\n",
    "file_id =  'digits/5752'\n",
    "focus_times = np.array([0.,1.0])\n",
    "type = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading a TIMIT test file\n",
    "# choose an example from 0 .. 4 \n",
    "file_id = 3\n",
    "timit_focus_times = [ [2.10,3.07], [.12,.88], [.05,.51], [0.,.85], [1.26,2.65] ] # [0.,1], [0.,1], [0.,1.]]\n",
    "focus_times = np.array(timit_focus_times[file_id])\n",
    "type = \"TIMIT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all the data\n",
    "wavdata, sr, spgdata, ftrs, txt, seg_wrd, seg_phn = get_test_file(file_id,feature_args=feature_args,type=type)\n",
    "print(txt,seg_wrd)\n",
    "transcript = seg_phn['seg'].tolist()\n",
    "# select a time segment to focus on and find corresponding phones / transcription\n",
    "labels = pyspch.seg2lbls(seg_phn, n_frames=ftrs.shape[1],pad_lbl='sil')\n",
    "shift=feature_args['f_shift']\n",
    "#  SHOW THE EXAMPLE: waveform, spectrogram, features, reference transcription and segmentation\n",
    "##############################################################################################\n",
    "fig = pyspch.display.PlotSpgFtrs(wavdata=wavdata, spgdata=spgdata,dy=1,\n",
    "                                  sample_rate=sr, figsize=(15,6), img_ftrs=[ftrs])\n",
    "fig.add_seg_plot(seg_phn, iax=0, ypos=.1, color='r',size=14) \n",
    "fig.add_seg_plot(seg_wrd, iax=0, ypos=.9, color='k',size=14) \n",
    "display(fig)\n",
    "display(Audio(data=wavdata,rate=sr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time & Segment selection for plots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#focus_times = np.array([0.51,.92])\n",
    "frames= (focus_times/shift).astype('int')\n",
    "samples = (focus_times*sr).astype('int')\n",
    "sample_sel = slice(samples[0],samples[1])\n",
    "frame_sel = slice(frames[0],frames[1])\n",
    "seg_sel = seg_phn[(seg_phn['t0']<frames[1]*shift) & (seg_phn['t1']>frames[0]*shift)]\n",
    "state_sel = np.array(seg_sel.index.tolist())\n",
    "transcript_sel = np.array(transcript)[state_sel]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frame Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gmm_model\n",
    "nftrs = 26\n",
    "#model = dnn_model\n",
    "#nftrs = 39\n",
    "X=ftrs[:nftrs,:].T\n",
    "frame_proba = model.predict_proba(X)\n",
    "# normalized frame \"observation\" likelihoods can be computed fromt the posterior probabilities \n",
    "frame_probs = proba_2_prob(frame_proba,priors=gmm_model.class_prior_)\n",
    "frame_logprobs = Spch.logf(frame_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frame Recognition\n",
    "\n",
    "In frame recognition each feature vector is treated independently.  It consists of 2 simple steps:   \n",
    "1. compute the class probabilities for each feature vector\n",
    "2. recognize by finding the maximum amongst all classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute frame posterior probabilities and do frame by frame recognition, compute the frame error rate in the sentence\n",
    "##########################################################################\n",
    "# evaluate frame based recognition wrt. manual segmentations\n",
    "y_frame_idx = np.argmax(frame_proba, axis=1)\n",
    "#y_frame_idx = np.argmax(frame_probs, axis=1)\n",
    "y_frame_lab = [phn_set[i] for i in y_frame_idx]\n",
    "t = np.arange(len(y_frame_lab))*shift+0.5*shift\n",
    "y_frame_ldf = pd.DataFrame({'t':t,'lbl':y_frame_lab})\n",
    "y_frame_seg = pyspch.lbls2seg(y_frame_lab)\n",
    "n_corr = np.sum([y_frame_lab[i] == labels[i] for i in range(len(labels))])\n",
    "print(\"Frame Recognition Rate (for full utterance): %.2f %%\" % (100.*n_corr/len(labels)))\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the outputs of the most likely phones and at least all phones in the transcript, resorted for order in alphabet\n",
    "top_k = 10\n",
    "indx_sel = np.argsort(np.sum(frame_proba,axis=0))[::-1][0:top_k]\n",
    "indx_sel = np.sort(indx_sel)\n",
    "phn_sel = [phn_set[i] for  i in indx_sel]\n",
    "phn_sel,indx_sel = select_phones_idx((phn_sel,transcript_sel),phn_set,SORTED=False)\n",
    "#\n",
    "fig = pyspch.display.PlotSpgFtrs(wavdata=wavdata, spgdata=ftrs,spglabel=\"\",dy=1,row_heights=[1,1,3,.5],\n",
    "            sample_rate=sr, figsize=(16,9), frames=frames, img_ftrs=[None,None],img_labels=[None,None])\n",
    "\n",
    "(iax_wav, iax_ftr, iax_prob, iax_frame) = (0,1,2,3)\n",
    "fig.add_seg_plot(seg_phn, iax=iax_wav, ypos=.1, color='r',size=14) \n",
    "fig.add_seg_plot(seg_wrd, iax=iax_wav, ypos=.9, color='k',size=14)\n",
    "fig.axes[iax_ftr].set_title(\"MFCC39\")\n",
    "fig.axes[iax_ftr].set_xticks([])\n",
    "plot_probs(frame_proba[frame_sel,indx_sel], phn_sel,fig=fig, cmap='Greys',iax=iax_prob, style=\"img\",title=\"Phone Probabilities per frame\") \n",
    "#fig.axes[iax_prob].set_ylabel('Frame Probabilities')\n",
    "fig.add_seg_plot(y_frame_ldf,iax=iax_frame,ypos=.5,color='#00F',size=10,txtargs={'rotation':75},title=\"Frame Recognition\")\n",
    "\n",
    "#fig.axes[iax_frame].axis('off')\n",
    "fig.axes[iax_frame].set_xticks([])\n",
    "fig.axes[iax_frame].set_xlabel(\"\")\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viterbi Alignment\n",
    "For a Viterbi alignment we use a Trellis setup.\n",
    "If we want to align our data with a given transcription, then we may enforce a strict left-to-right arrangement of HMM states corresponding to the transcription.\n",
    "\n",
    "It consists of 3 steps:   \n",
    "1. compute the observation probabilities for each state given the feature vector (we compute likelihoods from the probabilities used for frame recognition)\n",
    "2. complete the trellis (left to right) with the state sequence on the vertical and frame likelihoods on the horizontal axis .  Note that the HMM model uses 'likelihoods' and '(posterior) probabilities'\n",
    "3. Find the Viterbi alignment by backtracking on the trellis\n",
    "\n",
    "In the end we compute the frame level agreement with our given 'manual' alignment.  This will likely NOT be 100%.  But this is not problematic.\n",
    "The manual alignment is probably not perfect.  Overall the obtained alignment is of high quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viterbi alignment with feeding the observation probabilities directly to the model\n",
    "l2r = 1\n",
    "skip=0\n",
    "obs_indx, trans_mat,init_mat,end_states = make_graph(transcript,phn_set,l2r=l2r,skip=skip)\n",
    "#####\n",
    "# alternative with features as input to the libhmm routines\n",
    "# observations = X\n",
    "# model = model\n",
    "####\n",
    "model = None\n",
    "observations = frame_logprobs\n",
    "hmm1 = libhmm.HMM(prob_style=\"lin\",obs_indx=obs_indx,obs_model=model,states=transcript,transmat=trans_mat,initmat=init_mat,end_states=end_states)\n",
    "hmm1.set_probstyle('log')\n",
    "trellis1=libhmm.Trellis(hmm1,Normalize=True)\n",
    "trellis1.viterbi_pass(observations)\n",
    "# recognize from the trellis\n",
    "y_vit_lbl = hmm1.states[trellis1.backtrace()]\n",
    "y_vit_ldf = pd.DataFrame({'t':t,'lbl':y_vit_lbl})\n",
    "y_vit_seg = Spch.lbls2seg(y_vit_lbl)\n",
    "n_corr = np.sum([y_vit_lbl[i] == labels[i] for i in range(len(y_vit_lbl))])\n",
    "print(\"Frame level agreement with manual alignment: %.2f %%\" % (100.*n_corr/len(y_vit_lbl)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "fig = pyspch.display.PlotSpgFtrs(wavdata=wavdata, spgdata=ftrs,spglabel=\"\",dy=1,row_heights=[1,1,3,3,1],\n",
    "            frames=frames, sample_rate=sr, figsize=(16,9), img_ftrs=[None,None],img_labels=[None,None])\n",
    "#\n",
    "(iax_wav, iax_ftr, iax_prob,  iax_trel, iax_vit ) = (0,1,2,3,4)\n",
    "fig.add_seg_plot(seg_phn, iax=iax_wav, ypos=.1, color='r',size=14) \n",
    "fig.add_seg_plot(seg_wrd, iax=iax_wav, ypos=.9, color='k',size=14)\n",
    "fig.axes[iax_ftr].set_xticks([])\n",
    "fig.axes[iax_ftr].set_title(\"MFCC39\")\n",
    "#\n",
    "log_probs = np.log(frame_proba) \n",
    "plot_probs(log_probs[frame_sel,indx_sel],phn_sel,fig=fig, cmap='Greys',iax=iax_prob,title=\"Frame Probabilities (log, normalized)\",\n",
    "          vmin=-10, style=\"img\" ) # style=\"lin\",yrange=[-10.,0]  )\n",
    "#\n",
    "trel_probs = trellis1.probs\n",
    "plot_probs(trel_probs[frame_sel,state_sel],fig=fig,cmap='coolwarm',labels=transcript_sel,iax=iax_trel,title=\"Trellis Probabilities (log, normalized)\",\n",
    "          style=\"img\",vmin=-100.) \n",
    "fig.add_seg_plot(y_vit_seg,iax=iax_vit,ypos=.5,color='#00F',size=14,title=\"Viterbi Alignment\")\n",
    "#\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phone Recognition \n",
    "We can use our trellis to do recognition as well, i.e. to find the best possible state sequence that might underly the data.\n",
    "In order to use the trellis for recognition, we need to define all possible transition probabilities between states.\n",
    "In this notebook we use single state phone models, making the graph construction rather trivial.\n",
    "The graph construction module that we use here allows to specify probabilities for ergodic (any-to-any) transitions, the left-to-right transitions\n",
    "and skip transitions.  The self-loop probability is set to '1.0'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2r = 0.\n",
    "skip=0.\n",
    "eps=.001\n",
    "trel_phones = np.unique(transcript) # transcript\n",
    "trel_indx = [ phn2idx[ph] for ph in trel_phones ]\n",
    "obs_indx, trans_mat,init_mat,end_states = make_graph(trel_phones,phn_set,eps=eps,skip=skip,style=\"RECO\")\n",
    "#\n",
    "# for the HMM we use likelihoods instead of posteriors\n",
    "hmm2 = libhmm.HMM(prob_style=\"lin\",obs_indx=obs_indx,obs_model=None,states=trel_phones,transmat=trans_mat,initmat=init_mat,end_states=end_states)\n",
    "hmm2.set_probstyle('log')\n",
    "trellis2=libhmm.Trellis(hmm2,Normalize=True)\n",
    "trellis2.viterbi_pass(frame_logprobs)\n",
    "# recognize from the trellis\n",
    "y_reco_lbl = hmm2.states[trellis2.backtrace()]\n",
    "y_reco_ldf = pd.DataFrame({'t':t,'lbl':y_reco_lbl})\n",
    "y_reco_seg = Spch.lbls2seg(y_reco_lbl)\n",
    "n_corr = np.sum([y_reco_lbl[i] == labels[i] for i in range(len(y_reco_lbl))])\n",
    "print(\"Agreement with manual alignment: %.2f %%\" % (100.*n_corr/len(y_reco_lbl)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspch import dtw\n",
    "y = np.array(seg_phn['seg'])\n",
    "x = np.array(y_reco_seg['seg'])\n",
    "lev_dist,trace,ld,cd,bptrs = dtw.dtw(x,y,trans=\"LEV\",p='hamming',result='details')\n",
    "print(\"Levenshtein Distance:  %d / %d \" %(lev_dist,len(y)))\n",
    "print(\"Phone Error Rate:  %.2f%% \" %(100.*lev_dist/len(y)))\n",
    "print(y), print(x)\n",
    "plt_args = {'ftr_scale':.05,'fig_width':6.,'ftr_args':{'alpha':.2,'cmap':'Greys','fontweight':'bold'}, \n",
    "           'xy_annot':True,'xy_args':{'cmap':'coolwarm','alpha':.5,'text_size':'x-small'} }\n",
    "display(dtw.plot_trellis(x=x,y=y,xy_mat=cd,trace=trace,bptrs=bptrs,**plt_args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "fig = pyspch.display.PlotSpgFtrs(wavdata=wavdata, spgdata=ftrs,spglabel=\"\",dy=1,row_heights=[1,1,3,3,1],\n",
    "            frames=frames, sample_rate=sr, figsize=(16,9), img_ftrs=[None,None],img_labels=[None,None])\n",
    "#\n",
    "(iax_wav, iax_ftr, iax_prob,  iax_trel, iax_vit ) = (0,1,2,3,4)\n",
    "fig.add_seg_plot(seg_phn, iax=iax_wav, ypos=.1, color='r',size=14) \n",
    "fig.add_seg_plot(seg_wrd, iax=iax_wav, ypos=.9, color='k',size=14)\n",
    "fig.axes[iax_ftr].set_title(\"MFCC39\")\n",
    "fig.axes[iax_ftr].set_xticks([])\n",
    "#\n",
    "log_probs = np.log(frame_proba) \n",
    "plot_probs(log_probs[frame_sel,indx_sel],phn_sel,vmin=-10.,fig=fig, cmap='Greys',iax=iax_prob, style=\"img\",title=\"Frame Probabilities (log, normalized)\") \n",
    "#\n",
    "trel_probs = trellis2.probs\n",
    "plot_probs(trel_probs[frame_sel,:],labels=trel_phones,vmin=-30.,fig=fig,cmap='coolwarm',iax=iax_trel,style=\"img\",title=\"Trellis Probabilities (log, normalized)\",\n",
    "           annot=False)\n",
    "#fig.add_seg_plot(y_reco_ldf,iax=iax_vit,ypos=.75,color='#000',size=10)\n",
    "fig.add_seg_plot(y_reco_seg,iax=iax_vit,ypos=.5,color='#00F',size=14,title=\"Phone Recognition\")\n",
    "#fig.axes[iax_vit].set_ylabel('Recognition')\n",
    "#\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Selected Segments and Phones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a selection of phones to plot\n",
    "# include the most likely outputs and the phones in the transcriptions\n",
    "top_k = 10\n",
    "indx_sel = np.argsort(np.sum(frame_proba[frame_sel,:],axis=0))[::-1][0:top_k]\n",
    "phn_sel = [phn_set[i] for i in indx_sel]\n",
    "phn_sel, indx_sel = select_phones_idx((seg_sel['seg'],phn_sel,['sil']),phn_set,SORTED=True)\n",
    "\n",
    "# plot frame posterior probs and phones for a short segment and a selection of phones\n",
    "######################################################################################\n",
    "fig = pyspch.display.PlotSpgFtrs(wavdata=wavdata, spgdata=spgdata,dy=1,row_heights=[1.5,2,2,5,1],\n",
    "            frames=frames, sample_rate=sr, figsize=(16,9), img_ftrs=[ftrs,None,None],img_labels=['Features',None,None])\n",
    "(iax_wav, iax_spg, iax_ftr, iax_prob, iax_frame, iax_ref) = (0,1,2,3,4,5)\n",
    "fig.add_seg_plot(seg_phn, iax=iax_wav, ypos=.1, color='r') # seg\n",
    "fig.add_seg_plot(seg_wrd, iax=iax_wav, ypos=.9, Lines=True, color='r')\n",
    "\n",
    "plot_probs(frame_proba[frame_sel,indx_sel], phn_sel,fig=fig, cmap='coolwarm',iax=iax_prob, vmin=None, x0=frames[0]*shift, style=\"img\", title=\"Frame Probabilities\")\n",
    "fig.add_seg_plot(y_frame_ldf,iax=iax_frame,ypos=.5,color='#00F',size=10)\n",
    "fig.axes[iax_frame].set_ylabel('Frame\\nRecog')\n",
    "\n",
    "display(fig)\n",
    "n_corr = np.sum([y_frame_lab[i] == labels[i] for i in range(frames[0],frames[1])])\n",
    "n_frames = (frames[1]-frames[0])\n",
    "print(\"Frame Recognition Rate: %d/%d (%.2f%%)\" % (n_corr,n_frames,100.*n_corr/n_frames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "fig = Spd.PlotSpgFtrs(wavdata=wavdata,spgdata=ftrs,spglabel=\"MFCC39\",dy=1,frames=frames,sample_rate=sr,figsize=(15,10),row_heights=[1,1,4,1,4,1,1],\n",
    "                  img_ftrs=[None,None,None])\n",
    "(iax_wav, iax_spg, iax_prob, iax_frame, iax_trel, iax_vit, iax_ref) = (0,1,2,3,4,5,6)\n",
    "obs_probs = np.log(frame_proba) \n",
    "obs_probs = frame_proba\n",
    "trel_probs = trellis1.probs\n",
    "plot_probs(obs_probs[frame_sel,indx_sel],fig=fig,iax=iax_prob,labels=phn_sel,cmap=\"Greys\",style=\"img\",title=\"Observation Probabilities (log, normalized)\")\n",
    "plot_probs(trel_probs[frame_sel,state_sel],fig=fig,cmap='coolwarm',labels=transcript_sel,iax=iax_trel,style=\"img\",title=\"Trellis Probabilities (log, normalized)\",annot=True)\n",
    "#fig.add_seg_plot(seg_wrd,iax=0,ypos=0.8,color='k')\n",
    "\n",
    "#fig.add_seg_plot(y_vit_seg,iax=4,ypos=0.5,color='b',title='Viterbi Alignment')\n",
    "\n",
    "fig.add_seg_plot(y_frame_ldf,iax=iax_frame,ypos=.5,color='#00F',size=10)\n",
    "fig.add_seg_plot(y_vit_ldf,iax=iax_vit,ypos=.75,color='#000',size=10)\n",
    "fig.add_seg_plot(y_vit_seg,iax=iax_vit,ypos=.25,color='#00F',size=14)\n",
    "fig.add_seg_plot(seg_phn,iax=iax_ref,ypos=.25,color='r',size=14,Lines=False)\n",
    "fig.axes[iax_frame].set_ylabel('FRAME')\n",
    "fig.axes[iax_vit].set_ylabel('VIT')\n",
    "fig.axes[iax_ref].set_ylabel('REF',color='r')\n",
    "fig.axes[iax_frame].set_xlabel(\"\")\n",
    "#fig.add_seg_plot(seg_phn,iax=iax_ref,ypos=0.2,color='b',title='Reference Alignment')\n",
    "display(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "i1 = 60\n",
    "i2 = 70\n",
    "trellis = trellis1\n",
    "obs_probs = frame_logprobs\n",
    "trel_probs = trellis.probs\n",
    "dfo1 = pd.DataFrame(obs_probs[i1:i2,indx_sel].T,index=phn_sel,columns=np.arange(i1,i2))\n",
    "dft1 = pd.DataFrame(trel_probs[i1:i2,state_sel].T,index=transcript_sel,columns=np.arange(i1,i2))\n",
    "dfo2 = pd.DataFrame(obs_probs[i1:i2,:].T,index=phn_set,columns=np.arange(i1,i2))\n",
    "dft2 = pd.DataFrame(trel_probs[i1:i2,:].T,index=transcript,columns=np.arange(i1,i2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dfo1, dft1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2,figsize=(12,6))\n",
    "\n",
    "kwargs = {\"cmap\":\"coolwarm\",\"fmt\":\".2f\",\"linewidths\":1,\"linecolor\":'k',\"annot\":True,\"annot_kws\":{\"fontsize\":8},'cbar':False,'square':False}\n",
    "sns.heatmap(dfo1,ax=ax[0],**kwargs)\n",
    "sns.heatmap(dft1,ax=ax[1],**kwargs)\n",
    "#cmap=\"coolwarm\",fmt=\".2f\",linewidths=1,linecolor='k',annot=True) #, ax=ax, yticklabels=labels, cmap=cmap,vmin=vmin,,cbar_kws={'pad':0.0})\n",
    "for axx in ax: axx.set_yticklabels(axx.get_yticklabels(), rotation=0)\n",
    "#ax.set_xticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trellis1.plot_trellis(figsize=(16,8),vmin=-100,plot_values=False,plot_backptrs=True,plot_alignment=True,cmap=\"coolwarm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trellis.probs[0:5].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trellis.obs_probs[0:5].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trellis.backptrs[0:5].T"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ex_timit-3",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
