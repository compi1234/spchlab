{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## CLASSIFICATION of SPEECH FRAMES\n",
    "\n",
    "### PART IV:  Recognize Vowels from FBANK features using DNN Classifiers in PyTorch"
   ],
   "metadata": {
    "id": "xiSK1LBqqxbL"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. Setting up your Python Environment\n",
    " \n",
    "+ Import Python's Machine Learning Stack\n",
    "+ Import needed local utilities that are needed for this exercise\n",
    "+ Import Pytorch"
   ],
   "metadata": {
    "id": "nEBOGxM441mY"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%matplotlib inline\r\n",
    "import sys,os,io\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import pandas as pd\r\n",
    "import seaborn as sns\r\n",
    "import scipy.io as sio\r\n",
    "import urllib.request\r\n",
    "\r\n",
    "# imports from the scikit-learn \r\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\r\n",
    "from sklearn.linear_model import Perceptron\r\n",
    "from sklearn import metrics as skmetrics\r\n",
    "from sklearn.mixture import GaussianMixture\r\n",
    "from scipy.fftpack import dct\r\n",
    "\r\n",
    "# import pytorch\r\n",
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "from torch.utils.data import Dataset, DataLoader\r\n",
    "\r\n",
    "try:\r\n",
    "  import google.colab\r\n",
    "  IN_COLAB = True\r\n",
    "  ! pip install git+https://github.com/compi1234/pyspch.git\r\n",
    "except:\r\n",
    "  IN_COLAB = False\r\n",
    "\r\n",
    "from pyspch.io.hillenbrand import fetch_hillenbrand, select_hillenbrand\r\n"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16446,
     "status": "ok",
     "timestamp": 1634903354628,
     "user": {
      "displayName": "Bob Van Dyck",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09704130763050227218"
     },
     "user_tz": -120
    },
    "id": "r-5Fz07p41mZ",
    "outputId": "ca7c860f-cce5-4a34-d308-a639dccce3ec"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# choose the colors you like :)\r\n",
    "palette = sns.color_palette(\"bright\")\r\n",
    "# palette=['red','green','blue','orange','brown','black','dodgerblue','mediumturquoise','cyan','violet','gold','salmon'] \r\n",
    "# sns.palplot(palette)\r\n",
    "sns.set_palette(palette)"
   ],
   "outputs": [],
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1634903354629,
     "user": {
      "displayName": "Bob Van Dyck",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09704130763050227218"
     },
     "user_tz": -120
    },
    "id": "KabZnIBG41md"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. The Database \n",
    "The experiments in this notebook use a subset of the TIMIT database.\n",
    "Instead of using raw speech, we extract features from the speech signal.\n",
    "In what follows we will asses how \n",
    "There is FILTERBANK data from 3 vowels (i,a,uw) , 400 samples for training and 200 samples for testing.\n",
    "The data is 24-dimensional (24 channels in the filterbank).\n",
    "http://homes.esat.kuleuven.be/~spchlab/datasets/tinytimit/README.txt\n",
    "\n",
    "The loaded data consists of:\n",
    "- data matrices   FBANK_train(2400,3), FBANK_test(600,3)\n",
    "- labels          y_train(2400,), y_test(600,)"
   ],
   "metadata": {
    "id": "LIjZCTLg41mf"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# loads all data in a matlab file at given url to the contents structure\r\n",
    "# this is working for MATLAB 7.0 files and older ; not hdf5 MATLAB 7.3 or more recent\r\n",
    "def load_matlab_from_url(url):\r\n",
    "    url_response = urllib.request.urlopen(url)\r\n",
    "    matio = io.BytesIO(url_response.read())\r\n",
    "    contents = sio.loadmat(matio,squeeze_me=True)\r\n",
    "    return(contents)\r\n",
    "\r\n",
    "# we will import 400 train samples and 200 test samples for 3 vowels\r\n",
    "tinytimit = 'http://homes.esat.kuleuven.be/~spchlab/datasets/tinytimit/'\r\n",
    "url_mf = tinytimit + 'male-female.mat' \r\n",
    "data_mf = load_matlab_from_url(url_mf)\r\n",
    "url_vow3= tinytimit + 'a-i-uw-800.mat' \r\n",
    "data_vow3 = load_matlab_from_url(url_vow3)\r\n",
    "\r\n",
    "# labels\r\n",
    "y_train =np.full((2400,),'a',dtype='<U2')\r\n",
    "y_train[800:1600] =np.full((800,),'i',dtype='<U2')\r\n",
    "y_train[1600:2400] =np.full((800,),'uw',dtype='<U2')\r\n",
    "y_test =np.full((600,),'a',dtype='<U2')\r\n",
    "y_test[200:400] =np.full((200,),'i',dtype='<U2')\r\n",
    "y_test[400:600] =np.full((200,),'uw',dtype='<U2')\r\n",
    "classes = np.unique(y_train)"
   ],
   "outputs": [],
   "metadata": {
    "executionInfo": {
     "elapsed": 3580,
     "status": "ok",
     "timestamp": 1634903358200,
     "user": {
      "displayName": "Bob Van Dyck",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09704130763050227218"
     },
     "user_tz": -120
    },
    "id": "hOHNVsK741mg"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Feature Extraction\n",
    "\n",
    "For our NN-based phoneme-classifier, we can choose between three types of input features.  First, we have the filterbank energies (FBANK) directly. Second, we can use the PCA features derrived from the FBANK features. Alternatively MFCC features can be derived from the FBANK features."
   ],
   "metadata": {
    "id": "w5hyWXNXqS6x"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# A. Filterbank Energies (spectral features)\r\n",
    "FB_train=data_vow3['ALLtrain'].T\r\n",
    "FB_test=data_vow3['ALLtest'].T\r\n",
    "\r\n",
    "# B. PCA features (Principle Component Analysis)\r\n",
    "from sklearn.decomposition import PCA\r\n",
    "pca = PCA()\r\n",
    "T = pca.fit(FB_train)\r\n",
    "PCA_train = T.transform(FB_train)\r\n",
    "PCA_test = T.transform(FB_test)\r\n",
    "\r\n",
    "# C. Mel-Frequency Cepstral Coefficients\r\n",
    "MFCC_train = dct(FB_train, type=2, axis=1, norm='ortho')\r\n",
    "MFCC_test = dct(FB_test, type=2, axis=1, norm='ortho')"
   ],
   "outputs": [],
   "metadata": {
    "executionInfo": {
     "elapsed": 275,
     "status": "ok",
     "timestamp": 1634897167269,
     "user": {
      "displayName": "Bob Van Dyck",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09704130763050227218"
     },
     "user_tz": -120
    },
    "id": "jZPRL9cY41mj"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Feature visualization"
   ],
   "metadata": {
    "id": "RdVlR9TSqNXq"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Define a number of different Features\r\n",
    "\r\n",
    "# A. Filterbank Energies (spectral features)\r\n",
    "dfX = pd.DataFrame(FB_train[:,0:4])\r\n",
    "dfy = pd.Series(y_train, name='vowel')\r\n",
    "FB_df = pd.concat([dfy, dfX], axis=1)\r\n",
    "\r\n",
    "# B. PCA features (Principle Component Analysis)\r\n",
    "dfX = pd.DataFrame(PCA_train[:,0:4])\r\n",
    "dfy = pd.Series(y_train, name='vowel')\r\n",
    "PCA_df = pd.concat([dfy, dfX], axis=1)\r\n",
    "\r\n",
    "# C. Mel-Frequency Cepstral Coefficients\r\n",
    "dfX = pd.DataFrame(MFCC_train[:,0:4])\r\n",
    "dfy = pd.Series(y_train, name='vowel')\r\n",
    "MFCC_df = pd.concat([dfy, dfX], axis=1)"
   ],
   "outputs": [],
   "metadata": {
    "executionInfo": {
     "elapsed": 275,
     "status": "ok",
     "timestamp": 1634904310326,
     "user": {
      "displayName": "Bob Van Dyck",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09704130763050227218"
     },
     "user_tz": -120
    },
    "id": "U7roBFLw7T8D"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# choose feature set to use in plots and experiments\r\n",
    "df = MFCC_df\r\n",
    "df.columns = [\"vowel\",\"0\",\"1\",\"2\",\"3\"]\r\n",
    "\r\n",
    "doPlot = True\r\n",
    "if doPlot:\r\n",
    "  # plot 2 first dimensions of feature \r\n",
    "  f=plt.figure(figsize=(10, 10))\r\n",
    "  sns.scatterplot(data=df, x=\"0\", y=\"1\", hue='vowel')\r\n",
    "\r\n",
    "  # plot all dimensions pairwise\r\n",
    "  g = sns.PairGrid(df, hue=\"vowel\")\r\n",
    "  g.map_diag(plt.hist, histtype=\"step\", linewidth=1)\r\n",
    "  g.map_offdiag(plt.scatter, s=1)\r\n",
    "  g.add_legend()"
   ],
   "outputs": [],
   "metadata": {
    "executionInfo": {
     "elapsed": 245,
     "status": "ok",
     "timestamp": 1634904312020,
     "user": {
      "displayName": "Bob Van Dyck",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09704130763050227218"
     },
     "user_tz": -120
    },
    "id": "iBeYPHlx8UYu"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. Setting up a Neural Network in Pytorch\n",
    "\n",
    "Quite a few helper routines are defined for learning our Neural Network in Pytorch.\n",
    "There is no need to go through this in detail, but interested people can of course do this.\n",
    "\n",
    "There are 3 blocks of code:\n",
    "- definition of classes for the Neural Net and the Data Sets\n",
    "- training routines\n",
    "- evalutation routines"
   ],
   "metadata": {
    "id": "XkQsiCUpDfzW"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# =============================================================================\r\n",
    "# Define Classes for Neural network architecture and Data Sets\r\n",
    "# =============================================================================\r\n",
    "\r\n",
    "# simple feedforward neural network \r\n",
    "class simple_ffnn(nn.Module):\r\n",
    "    \r\n",
    "    def __init__(self, in_dim, out_dim, hidden_layer_sizes):\r\n",
    "        super(simple_ffnn, self).__init__()\r\n",
    "\r\n",
    "        # attributes\r\n",
    "        self.in_dim = in_dim\r\n",
    "        self.out_dim = out_dim\r\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\r\n",
    "\r\n",
    "        # parameters\r\n",
    "        layer_sizes = (in_dim, *hidden_layer_sizes, out_dim)\r\n",
    "        layer_sizes_pairwise = [(layer_sizes[i], layer_sizes[i+1]) for \r\n",
    "                                 i in range(len(layer_sizes)-1)]\r\n",
    "\r\n",
    "        # define architecture\r\n",
    "        modulelist = nn.ModuleList([])\r\n",
    "        for layer_in_size, layer_out_size in layer_sizes_pairwise:\r\n",
    "            modulelist.append(nn.Linear(layer_in_size, layer_out_size))\r\n",
    "            modulelist.append(nn.Sigmoid())\r\n",
    "\r\n",
    "        # define network as nn.Sequential\r\n",
    "        self.net = nn.Sequential(*modulelist)\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        x = self.net(x)\r\n",
    "        return x\r\n",
    "        \r\n",
    "    def init_weights(self):\r\n",
    "        self.net.apply(init_normal) \r\n",
    "\r\n",
    "def init_normal(m):\r\n",
    "    if type(m) == nn.Linear:\r\n",
    "        nn.init.kaiming_uniform_(m.weight)   \r\n",
    "        \r\n",
    "# Dataset \r\n",
    "class SimpleDataset(Dataset):\r\n",
    "    \"\"\"Simple dataset for easy sampling.\"\"\"\r\n",
    "\r\n",
    "    def __init__(self, data_X, data_y, labels, labeldict, device):\r\n",
    "\r\n",
    "        # dimensionality\r\n",
    "        self.n_samples, self.n_features = data_X.shape\r\n",
    "        self.n_classes = len(labels)\r\n",
    "\r\n",
    "        # input data\r\n",
    "        self.frames = data_X # (n_samples, n_features)\r\n",
    "        self.frames = torch.as_tensor(self.frames, dtype=torch.float32).to(device)\r\n",
    "\r\n",
    "        # labels\r\n",
    "        if data_y.dtype != \"int64\":\r\n",
    "            data_y = np.vectorize(labeldict.get)(data_y)\r\n",
    "        self.labels = torch.as_tensor(data_y, dtype=torch.long).to(device)\r\n",
    "\r\n",
    "    def __len__(self):\r\n",
    "        \r\n",
    "        return self.n_samples\r\n",
    "\r\n",
    "    def __getitem__(self, idx):\r\n",
    "        frame = self.frames[idx] \r\n",
    "        label = self.labels[idx]\r\n",
    "        \r\n",
    "        return frame, label"
   ],
   "outputs": [],
   "metadata": {
    "executionInfo": {
     "elapsed": 252,
     "status": "ok",
     "timestamp": 1634897134228,
     "user": {
      "displayName": "Bob Van Dyck",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09704130763050227218"
     },
     "user_tz": -120
    },
    "id": "GqEWBZMzs_NO"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# =============================================================================\r\n",
    "# Neural network training routines\r\n",
    "# =============================================================================\r\n",
    "\r\n",
    "# simple batch gradient descent \r\n",
    "def train_batch(network, train_X, train_y, criterion, optimizer,\r\n",
    "                device, n_epochs=500, every=50):\r\n",
    "\r\n",
    "    # data-format is torch tensor + send to device (ex. GPU)\r\n",
    "    train_X = torch.tensor(train_X, dtype=torch.float32).to(device)\r\n",
    "    train_y = torch.tensor(train_y, dtype=torch.long).to(device)\r\n",
    "\r\n",
    "    # send network to device (ex. GPU)\r\n",
    "    network.to(device)\r\n",
    "\r\n",
    "    # set network to training mode (vs. evaluation mode)\r\n",
    "    network.train() \r\n",
    "\r\n",
    "    # save training loss\r\n",
    "    train_loss = []\r\n",
    "\r\n",
    "    # train for some epochs - full batch\r\n",
    "    for epoch in range(n_epochs):\r\n",
    "        optimizer.zero_grad()\r\n",
    "        output = network(train_X)\r\n",
    "        loss = criterion(output, train_y)\r\n",
    "        loss.backward()\r\n",
    "        optimizer.step()\r\n",
    "        if epoch%every == 0:\r\n",
    "          print('Epoch {}, loss {}'.format(epoch, loss.item()))\r\n",
    "        \r\n",
    "        train_loss.append(loss.item())\r\n",
    "      \r\n",
    "    return train_loss\r\n",
    "\r\n",
    "# mini-batch gradient descent\r\n",
    "def train_minibatch(network, train_dl, criterion, optimizer, device,\r\n",
    "                    n_epochs=500, every=50):\r\n",
    "    \"\"\"\r\n",
    "    Makes use of torch dataloader (see later on) to split data into mini-batches\r\n",
    "    \"\"\" \r\n",
    "\r\n",
    "    #  per epoch: update network parameters \r\n",
    "    for epoch in range(n_epochs):\r\n",
    "\r\n",
    "        # network to training mode\r\n",
    "        network.train()\r\n",
    "        \r\n",
    "        # save training loss\r\n",
    "        train_loss = []\r\n",
    "\r\n",
    "        # per mini-batch: compute gradient (backpropagation) + take step \r\n",
    "        running_loss = 0\r\n",
    "        steps = 0 \r\n",
    "        for i, data in enumerate(train_dl, 0):\r\n",
    "\r\n",
    "            # mini-batch inputs and labels\r\n",
    "            frames, labels = data  \r\n",
    "            \r\n",
    "            # zero the parameter gradients\r\n",
    "            optimizer.zero_grad()\r\n",
    "            \r\n",
    "            # forward + backward + optimize \r\n",
    "            outputs = network.net(frames)\r\n",
    "            loss = criterion(outputs, labels)\r\n",
    "            loss.backward()\r\n",
    "            optimizer.step()  \r\n",
    "\r\n",
    "            # running loss\r\n",
    "            running_loss += loss.item()\r\n",
    "\r\n",
    "        mean_minibatch_loss = running_loss/len(train_dl)\r\n",
    "        train_loss.append(mean_minibatch_loss)\r\n",
    "\r\n",
    "        if epoch%every == 0:   \r\n",
    "          print(\"Epoch %d -- av. loss per mini-batch %.2f\" % (epoch, mean_minibatch_loss))\r\n",
    "\r\n",
    "    return train_loss\r\n",
    "            \r\n",
    "# mini-batch gradient descent with early stopping\r\n",
    "def train_minibatch_earlystopping(network, train_dl, criterion, optimizer, device,\r\n",
    "                                  n_epochs=500, valid_X=None, valid_y=None, patience=5,\r\n",
    "                                  every=50):\r\n",
    "    \"\"\"\r\n",
    "    Makes use of torch dataloader (see later on) to split data into mini-batches\r\n",
    "    \"\"\" \r\n",
    "\r\n",
    "    # set early stopping counter\r\n",
    "    cnt_valid_loss_increase = 0\r\n",
    "\r\n",
    "    # data-format is torch tensor + send to device (ex. GPU)\r\n",
    "    valid_X = torch.tensor(valid_X, dtype=torch.float32).to(device)\r\n",
    "    valid_y = torch.tensor(valid_y, dtype=torch.long).to(device)\r\n",
    "\r\n",
    "    # save training and validation loss\r\n",
    "    train_loss_list = []\r\n",
    "    valid_loss_list = []\r\n",
    "\r\n",
    "    #  per epoch: update network parameters \r\n",
    "    for epoch in range(n_epochs):\r\n",
    "\r\n",
    "        # network to training mode\r\n",
    "        network.train()\r\n",
    "        \r\n",
    "        # early stopping \r\n",
    "        earlystoppping = cnt_valid_loss_increase > patience\r\n",
    "        if earlystoppping:\r\n",
    "            print(\"stopped early after %d epochs\" % (epoch))\r\n",
    "            return train_loss_list, valid_loss_list\r\n",
    "\r\n",
    "        # per mini-batch: compute gradient (backpropagation) + take step \r\n",
    "        running_loss = 0\r\n",
    "        steps = 0 \r\n",
    "        for i, data in enumerate(train_dl, 0):\r\n",
    "\r\n",
    "            # mini-batch inputs and labels\r\n",
    "            frames, labels = data  \r\n",
    "            \r\n",
    "            # zero the parameter gradients\r\n",
    "            optimizer.zero_grad()\r\n",
    "            \r\n",
    "            # forward + backward + optimize \r\n",
    "            outputs = network.net(frames)\r\n",
    "            loss = criterion(outputs, labels)\r\n",
    "            loss.backward()\r\n",
    "            optimizer.step()  \r\n",
    "\r\n",
    "            # running loss\r\n",
    "            running_loss += loss.item()\r\n",
    "\r\n",
    "        # training loss\r\n",
    "        mean_minibatch_loss = running_loss/len(train_dl)\r\n",
    "        train_loss_list.append(mean_minibatch_loss)\r\n",
    "        if epoch%every == 0:   \r\n",
    "            print(\"Epoch %d -- av. loss per mini-batch %.2f\" % (epoch, mean_minibatch_loss))\r\n",
    "\r\n",
    "        # validation loss\r\n",
    "        if valid_X is not None and valid_y is not None:          \r\n",
    "            valid_outputs = network.net(valid_X)\r\n",
    "            valid_loss = criterion(valid_outputs, valid_y)\r\n",
    "            valid_loss_list.append(valid_loss.item())\r\n",
    "            \r\n",
    "            # early stoppping\r\n",
    "            loss_increase = valid_loss_list[-1] > min(valid_loss_list[-patience:])\r\n",
    "            if loss_increase:\r\n",
    "                # no improvement compared to last 'patience' steps\r\n",
    "                cnt_valid_loss_increase += 1\r\n",
    "            else:\r\n",
    "                cnt_valid_loss_increase = 0\r\n",
    "\r\n",
    "    return train_loss_list, valid_loss_list"
   ],
   "outputs": [],
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1634897136311,
     "user": {
      "displayName": "Bob Van Dyck",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09704130763050227218"
     },
     "user_tz": -120
    },
    "id": "QtEjU9PgsIlq"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# =============================================================================\r\n",
    "# Neural network evaluation routines\r\n",
    "# =============================================================================\r\n",
    "\r\n",
    "# evaluate criterion\r\n",
    "def evaluate_criterion(network, test_X, test_y, criterion, device):\r\n",
    "    \r\n",
    "    # data-format is torch tensor + send to device (ex. GPU)\r\n",
    "    test_X = torch.tensor(test_X, dtype=torch.float32).to(device)\r\n",
    "    test_y = torch.tensor(test_y, dtype=torch.long).to(device)\r\n",
    "\r\n",
    "    # send network to device (ex. GPU)\r\n",
    "    network.to(device)\r\n",
    "\r\n",
    "    # set network to evaluation mode (vs. training mode)\r\n",
    "    network.eval() \r\n",
    "\r\n",
    "    # compute loss based on criterion\r\n",
    "    pred_y = network.net(test_X)\r\n",
    "    loss = criterion(pred_y, test_y)\r\n",
    "\r\n",
    "    return loss.item()\r\n",
    "\r\n",
    "# evaluate confusion matrix\r\n",
    "def confusionmatrix(network, test_X, test_y, device):\r\n",
    "    \r\n",
    "    # data-format is torch tensor + send to device (ex. GPU)\r\n",
    "    test_X = torch.tensor(test_X, dtype=torch.float32).to(device)\r\n",
    "    test_y = torch.tensor(test_y, dtype=torch.long).to(device)\r\n",
    "\r\n",
    "    # send network to device (ex. GPU)\r\n",
    "    network.to(device)\r\n",
    "\r\n",
    "    # set network to evaluation mode (vs. training mode)\r\n",
    "    network.eval() \r\n",
    "\r\n",
    "    # compute confusion matrix\r\n",
    "    cm = np.zeros((network.out_dim,network.out_dim))\r\n",
    "    for input, label in zip(test_X, test_y):\r\n",
    "        prob = network.net(input) # output = posterior class probabilities\r\n",
    "        pred = torch.argmax(prob) # prediction = label (or neuron) with highest probability (One-Hot Encoding)\r\n",
    "        cm[label][pred] += 1\r\n",
    "    \r\n",
    "    return cm.astype(int)\r\n",
    "\r\n",
    "# evaluate phone error rate\r\n",
    "def evaluate_PER(confusionmatrix):\r\n",
    "\r\n",
    "    n_classes = confusionmatrix.shape[0]    \r\n",
    "    \r\n",
    "    # compute ER \r\n",
    "    trace = np.trace(confusionmatrix)\r\n",
    "    ER = 1- trace.sum() / confusionmatrix.sum()\r\n",
    "        \r\n",
    "    # compute ER per class (disregarding non label or not)\r\n",
    "    no_examples_pc = confusionmatrix.sum(axis=1)\r\n",
    "    ER_pc = [None] * n_classes\r\n",
    "    for i in range(n_classes):\r\n",
    "        if no_examples_pc[i] != 0:\r\n",
    "            ER_pc[i] = 1-confusionmatrix[i,i] / (no_examples_pc[i])\r\n",
    "        \r\n",
    "    return ER, ER_pc\r\n",
    "\r\n",
    "# =============================================================================\r\n",
    "# Confusion matrix plot\r\n",
    "# =============================================================================\r\n",
    "# Pretty Print routine makes for confusion matrices\r\n",
    "def plot_confusion_matrix(cf_mat,labels=[],cmap='Blues'):\r\n",
    "    f,ax = plt.subplots()\r\n",
    "    sns.heatmap(cf_mat, annot=True,fmt=\"d\",square=True,\r\n",
    "                annot_kws={\"fontsize\": 14},xticklabels=labels,yticklabels=labels,\r\n",
    "                linecolor='k',linewidth=1.5,cmap=cmap,cbar=False)\r\n",
    "    ax.tick_params(axis='y',labelrotation=0.0,left=True)\r\n",
    "    plt.title('Confusion matrix')\r\n",
    "    plt.xlabel('Predicted')\r\n",
    "    plt.ylabel('True')\r\n",
    "    plt.show()"
   ],
   "outputs": [],
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1634897137387,
     "user": {
      "displayName": "Bob Van Dyck",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09704130763050227218"
     },
     "user_tz": -120
    },
    "id": "cNKJUp8Sr0Nt"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4. Experimental setup\n",
    "\n",
    "- Choose the feature extraction\n",
    "- Define Network Architecture"
   ],
   "metadata": {
    "id": "WLp0Vrj5sWdN"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Features (X) and labels (y) used for experiment\r\n",
    "\r\n",
    "# input data\r\n",
    "train_X = MFCC_train\r\n",
    "test_X = MFCC_test\r\n",
    "\r\n",
    "# define One-Hot Encoding for labels\r\n",
    "labels = [\"a\", \"i\", \"uw\"]\r\n",
    "labeldict = {\"a\" : 0, \"i\" : 1, \"uw\" : 2} \r\n",
    "inv_labeldict = {v : k for k, v in labeldict.items()}\r\n",
    "\r\n",
    "# encode labels\r\n",
    "train_y = np.vectorize(labeldict.get)(y_train).astype(np.int64)\r\n",
    "test_y = np.vectorize(labeldict.get)(y_test).astype(np.int64)"
   ],
   "outputs": [],
   "metadata": {
    "executionInfo": {
     "elapsed": 343,
     "status": "ok",
     "timestamp": 1634904313703,
     "user": {
      "displayName": "Bob Van Dyck",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09704130763050227218"
     },
     "user_tz": -120
    },
    "id": "rFQ6KEvMaDTR"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# network dimensions\r\n",
    "in_dim = train_X.shape[1]\r\n",
    "out_dim = len(classes)\r\n",
    "hidden_layer_sizes = [512,512]\r\n",
    "\r\n",
    "# set device\r\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "outputs": [],
   "metadata": {
    "executionInfo": {
     "elapsed": 213,
     "status": "ok",
     "timestamp": 1634904317296,
     "user": {
      "displayName": "Bob Van Dyck",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09704130763050227218"
     },
     "user_tz": -120
    },
    "id": "I0Tupeh0w2Xi"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Batch gradient descent"
   ],
   "metadata": {
    "id": "7Q0emNBmtsd1"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# batch gradient descent\r\n",
    "batch_model = simple_ffnn(in_dim=in_dim, out_dim=out_dim, hidden_layer_sizes=hidden_layer_sizes).to(device)\r\n",
    "\r\n",
    "# training setup\r\n",
    "n_epochs = 500\r\n",
    "batch_lrn_rate = 0.001\r\n",
    "batch_criterion = nn.CrossEntropyLoss()  # applies softmax()\r\n",
    "batch_optimizer = torch.optim.Adam(batch_model.parameters(), lr=batch_lrn_rate) # ties model-parameters to optimizer (back-propagation)\r\n",
    "\r\n",
    "# train network\r\n",
    "batch_train_loss= train_batch(batch_model, train_X, train_y, batch_criterion, batch_optimizer, device, n_epochs)\r\n",
    "\r\n",
    "# plot training loss\r\n",
    "plt.figure()\r\n",
    "plt.plot(batch_train_loss)\r\n",
    "plt.title(\"Training loss - batch gradient descent\")"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 468
    },
    "executionInfo": {
     "elapsed": 36742,
     "status": "ok",
     "timestamp": 1634904355560,
     "user": {
      "displayName": "Bob Van Dyck",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09704130763050227218"
     },
     "user_tz": -120
    },
    "id": "tKOzd3sxcDO5",
    "outputId": "368314e3-7cae-4c56-c3d7-fd769ccb9109"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Confusion matrix\r\n",
    "cm = confusionmatrix(batch_model, test_X, test_y, device).to(device)\r\n",
    "plot_confusion_matrix(cm, classes)\r\n",
    "\r\n",
    "# Phone Error Rate (PER) + PER per phone class\r\n",
    "per, per_pc = evaluate_PER(cm)\r\n",
    "print(\"PER %.2f and PER per phone class %s\" % (per, np.round(per_pc, 4)))\r\n",
    "\r\n",
    "# Cross-entropy loss\r\n",
    "print(\"CEL %.2f\" % evaluate_criterion(batch_model, test_X, test_y, batch_criterion, device))"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "executionInfo": {
     "elapsed": 652,
     "status": "ok",
     "timestamp": 1634904477040,
     "user": {
      "displayName": "Bob Van Dyck",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09704130763050227218"
     },
     "user_tz": -120
    },
    "id": "b4YYkT_urIXk",
    "outputId": "7855636d-31e3-455f-9097-341f2ee4229a"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Mini-batch gradient descent"
   ],
   "metadata": {
    "id": "mksQj9lmxFBV"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Dataset and Dataloader for easy sampling mini-batches\r\n",
    "\r\n",
    "# construct dataset\r\n",
    "train_ds = SimpleDataset(train_X, train_y, labels, labeldict, device)\r\n",
    "\r\n",
    "# construct dataloader\r\n",
    "batch_size = 32\r\n",
    "shuffle = True\r\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=shuffle, num_workers=0)"
   ],
   "outputs": [],
   "metadata": {
    "executionInfo": {
     "elapsed": 290,
     "status": "ok",
     "timestamp": 1634904480192,
     "user": {
      "displayName": "Bob Van Dyck",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09704130763050227218"
     },
     "user_tz": -120
    },
    "id": "Ovbiy5MGyC77"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# mini-batch gradient descent\r\n",
    "minibatch_model = simple_ffnn(in_dim=in_dim, out_dim=out_dim, hidden_layer_sizes=hidden_layer_sizes).to(device)\r\n",
    "\r\n",
    "# training setup\r\n",
    "n_epochs = 200\r\n",
    "mb_lrn_rate = 0.001\r\n",
    "mb_criterion = nn.CrossEntropyLoss()  # applies softmax()\r\n",
    "mb_optimizer = torch.optim.Adam(minibatch_model.parameters(), lr=mb_lrn_rate) # ties model-parameters to optimizer (back-propagation)\r\n",
    "\r\n",
    "# train network\r\n",
    "mb_train_loss = train_minibatch(minibatch_model, train_dl, mb_criterion, mb_optimizer, device, n_epochs)\r\n",
    "\r\n",
    "# plot training loss\r\n",
    "plt.figure()\r\n",
    "plt.plot(mb_train_loss)\r\n",
    "plt.title(\"Training loss - batch gradient descent\")\r\n"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 378
    },
    "executionInfo": {
     "elapsed": 70780,
     "status": "ok",
     "timestamp": 1634904552932,
     "user": {
      "displayName": "Bob Van Dyck",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "09704130763050227218"
     },
     "user_tz": -120
    },
    "id": "yNIhufu1w4Vj",
    "outputId": "1101c59f-c008-428c-e2a6-7d6d1db0f5ca"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Confusion matrix\r\n",
    "cm = confusionmatrix(minibatch_model, test_X, test_y, device)\r\n",
    "plot_confusion_matrix(cm, classes)\r\n",
    "\r\n",
    "# Phone Error Rate (PER) + PER per phone class\r\n",
    "per, per_pc = evaluate_PER(cm)\r\n",
    "print(\"PER %.2f and PER per phone class %s\" % (per, np.round(per_pc, 4)))\r\n",
    "\r\n",
    "# Cross-entropy loss\r\n",
    "print(\"CEL %.2f\" % evaluate_criterion(minibatch_model, test_X, test_y, batch_criterion, device))"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "executionInfo": {
     "elapsed": 137912,
     "status": "ok",
     "timestamp": 1607958619483,
     "user": {
      "displayName": "Bob Van Dyck",
      "photoUrl": "",
      "userId": "09704130763050227218"
     },
     "user_tz": -60
    },
    "id": "0b56P9BGxbGL",
    "outputId": "046994ca-0a14-4d2d-cffa-7bed9a1f7df2"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Mini-batch gradient descent with early stopping"
   ],
   "metadata": {
    "id": "51MbqnIWrCbU"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# test training method: mini-batch gradient descent\r\n",
    "minibatch_es_model = simple_ffnn(in_dim=in_dim, out_dim=out_dim, hidden_layer_sizes=hidden_layer_sizes).to(device)\r\n",
    "\r\n",
    "# training setup\r\n",
    "n_epochs = 200\r\n",
    "patience = 30\r\n",
    "mb_es_lrn_rate = 0.0001\r\n",
    "mb_es_criterion = nn.CrossEntropyLoss()  # applies softmax()\r\n",
    "mb_es_optimizer = torch.optim.Adam(minibatch_es_model.parameters(), lr=mb_es_lrn_rate) # ties model-parameters to optimizer (back-propagation)\r\n",
    "\r\n",
    "# train network\r\n",
    "mb_es_train_loss, mb_es_valid_loss = train_minibatch_earlystopping(minibatch_es_model, train_dl, mb_es_criterion, mb_es_optimizer, device, n_epochs, test_X, test_y, patience)\r\n",
    "\r\n",
    "# plot training loss\r\n",
    "plt.figure()\r\n",
    "plt.plot(mb_es_train_loss)\r\n",
    "plt.plot(mb_es_valid_loss)\r\n",
    "plt.title(\"Training loss - batch gradient descent\")"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 595
    },
    "executionInfo": {
     "elapsed": 194128,
     "status": "ok",
     "timestamp": 1607958675705,
     "user": {
      "displayName": "Bob Van Dyck",
      "photoUrl": "",
      "userId": "09704130763050227218"
     },
     "user_tz": -60
    },
    "id": "an5DeiJUmePh",
    "outputId": "bcff36a9-d5b0-46ed-bc12-4f811dc767b5"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Confusion matrix\r\n",
    "cm = confusionmatrix(minibatch_es_model, test_X, test_y, device)\r\n",
    "plot_confusion_matrix(cm, classes)\r\n",
    "\r\n",
    "# Phone Error Rate (PER) + PER per phone class\r\n",
    "per, per_pc = evaluate_PER(cm)\r\n",
    "print(\"PER %.2f and PER per phone class %s\" % (per, np.round(per_pc, 4)))\r\n",
    "\r\n",
    "# Cross-entropy loss\r\n",
    "print(\"CEL %.2f\" % evaluate_criterion(minibatch_es_model, test_X, test_y, batch_criterion, device))"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "executionInfo": {
     "elapsed": 194362,
     "status": "ok",
     "timestamp": 1607958675944,
     "user": {
      "displayName": "Bob Van Dyck",
      "photoUrl": "",
      "userId": "09704130763050227218"
     },
     "user_tz": -60
    },
    "id": "2QQnaL7poyAA",
    "outputId": "71665480-1c39-4cfa-de54-75fcedb3c9c8"
   }
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "tiny_timit_in_pytorch.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/compi1234/spchlab/blob/master/session3/timit-1.ipynb",
     "timestamp": 1601551199644
    }
   ]
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.11 64-bit ('spchlab': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  },
  "interpreter": {
   "hash": "679a7dd02d0c7f0c30c9a8c054f46b88a430cb091171e76a073ae0c75915d046"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}