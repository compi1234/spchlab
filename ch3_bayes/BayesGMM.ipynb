{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with Bayes Rule and a Generative Model (Gaussian Mixture Models)\n",
    "\n",
    "## Generative and Discriminative Models\n",
    "\n",
    "The standard approach to classify data from some feature observation $x$, is to compute a set of *discriminant functions* $f(C|x)$ for each class $C$ .  The class with maximum value will be recognised.\n",
    "Using the *posterior probabilities* $P(C|x)$ as discriminant functions furthermore guarantees minimization of the error rate.   \n",
    "However, this is easier said than done, as in most cases there exists no mathematically tractable solution to estimate those posterior probabilities.\n",
    "\n",
    "This has lead to two distinct classes of pattern classifiers.\n",
    "\n",
    "#### Discriminative Models\n",
    "\n",
    "In a discriminative system we learn the discriminant functions directly by minimizing the classification error on a given train set.\n",
    "\n",
    "#### Generative Models\n",
    "\n",
    "In generative models we compute the feature distributions of each class and by application of Bayes rule we then compute the posteriors:\n",
    "\n",
    "$ P(C|x) = \\frac{p(x|C) . P(C)}{ p(x) } $\n",
    "\n",
    "The generative model has a number of advantages and disadvantages:\n",
    "- the feature distributions can be estimated for each class independently, making a generative model in most cases computationally less demanding than a discriminative model\n",
    "- the feature distributions will be estimated well in high density regions; on the other hand in low density regions the estimates may be poor if not very poor.  Now, it is the contrast $p(x|C_1)$ vs. $p(x|C_2)$ that contributes most in the discrimination and what is high density for $C_1$ will be low density for $C_2$ and vice versa.  Thus, in all situations these poor low density estimates have a significant impact on the computation of the discriminant functions.\n",
    "\n",
    "#### Notations\n",
    "- $x$ is a D-Dimensional feature Vector\n",
    "- $C$ is a class label or index\n",
    "- $P(.)$  denotes a discrete probability\n",
    "- $p(.)$  denotes a probality density functions (or for density and probability at the same time)\n",
    "- $P(C|x)$ is the (posterior) probabilty for Class $C$ given feature vector $x$\n",
    "- $p(x|C)$ is the feature distribution of feature vector $x$ for class $C$   (Note: in discrete density models this may be used for the probability of the bin that x belongs )\n",
    "\n",
    "## Building a Classifier with GMMs\n",
    "\n",
    "In a generative model we must estimate the feature distributions per class with a priori unknown distribution.  A powerful approach exists in modeling the unknown distribution as a mixture of Gaussians. This type of model fits well with the assumption/observation that the features tend to cluster around one or a couple of centroids.  \n",
    "\n",
    "STEPS:\n",
    "1. Training the Feature Distributions $p(x|C)$\n",
    "\n",
    "- Group the (labeled) data per class\n",
    "- For each class: Fit a Gaussian Mixture to the train data \n",
    "\n",
    "$$ p(x|C_i) = \\sum_{j=1}^{M} w_{ij} \\mathcal{N}(x;\\mu_{ij},\\Sigma_{ij}) \n",
    "$$\n",
    "in which $ w_{ij}, \\mu_{ij}, \\Sigma_{ij}$  are respectively the weight, mean and convariance of mixture $j$ in class $C_i$.\n",
    "If not specified in more detail, it is fair to assume that a diagonal covariance matrix is being used. \n",
    "\n",
    "\n",
    "2. Construct the classifier\n",
    "\n",
    "+ For each class : compute the likelihoods $p(x|C)$ and weighted likelihoods $p(x|C) . P(C)$\n",
    "+ Compute the total likelihood of the feature vector: $p(x) = \\sum_c p(x|c) . P(c)$\n",
    "+ For each class: compute the posteriors: $ P(C|x) = \\frac{p(x|C) . P(C)}{ p(x) } $\n",
    "+ Find the class with maximum posterior probability\n",
    "\n",
    "Note: for plain classification we may omit the computation of $p(x)$ as it is identical over all classes and does not influence the maximisation operation.  On the other hand the exact posteriors are more intuitive and this simple normalization is not a computational burden to worry about.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
