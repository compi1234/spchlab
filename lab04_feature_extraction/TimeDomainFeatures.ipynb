{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/compi1234/spchlab/blob/main/lab04_feature_extaction/TimeDomainFeatures.ipynb\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Google Colab\" title=\"Open in Google Colab\"></a> \n",
    "# Time domain feature extraction\n",
    "\n",
    "In this notebook we explore feature extraction in the time domain; i.e. features that we can derive\n",
    "directly from the time-domain signal, without doing a frequency transformation first.  \n",
    "We focus on two conceptually simple features that are time varying and hence computed with a sliding window approach.\n",
    "\n",
    "_______________________________\n",
    "\n",
    "#### 1. Energy\n",
    "Energy ($E^2$) is defined as the average per sample energy in a short window.\n",
    "It is computed as the total energy in a frame (no windowing is applied) and which is then normalized for the number of samples in the frame, thus:   \n",
    "\n",
    "$E^2 =  \\frac{1}{N} \\sum_{i} x^2[i] $\n",
    "\n",
    "#### 2. Pitch\n",
    "For Pitch estimation we use the YIN algorithm as implemented in librosa.  It estimates the pitch period as the minimum of the difference function:   \n",
    "\n",
    "$d(\\tau) = \\sum_t (x(t) - x(t-\\tau)) $   \n",
    "\n",
    "$T = argmin_{\\tau}d(\\tau) $   \n",
    "\n",
    "The estimated pitch period is expressed in *(m)sec* and the pitch frequency (in *Hz*) is obtained by inversion of the pitch period.   \n",
    "\n",
    "$ f_0 = \\frac{1}{T} $   \n",
    "\n",
    "The above algorithm is simple and naive at the same time as human speech is never perfectly periodic.  Moreover non-periodic segments (unvoiced speech, silence, .. ) should be recognized as such.  Therefore a few additional heuristics are required to turn this baseline algorithm into an excellent state-of-the-art pitch estimator.\n",
    "\n",
    "Reference: *A. de Cheveigne et. al*, **YIN, a fundamental frequency estimator for speech and music** (http://audition.ens.fr/adc/pdf/2002_JASA_YIN.pdf)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the pip install command to install pyspch -- it is required!\n",
    "#\n",
    "#!pip install git+https://github.com/compi1234/pyspch.git\n",
    "#\n",
    "try:\n",
    "    import pyspch\n",
    "except ModuleNotFoundError:\n",
    "    try:\n",
    "        print(\n",
    "        \"\"\"\n",
    "        To enable this notebook on platforms as Google Colab, \n",
    "        install the pyspch package and dependencies by running following code:\n",
    "\n",
    "        !pip install git+https://github.com/compi1234/pyspch.git\n",
    "        \"\"\"\n",
    "        )\n",
    "    except ModuleNotFoundError:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os,sys, math\n",
    "import numpy as np\n",
    "\n",
    "import pyspch.sp as Sps\n",
    "import pyspch.core as Spch\n",
    "import pyspch.display as Spd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual, interactive, interactive_output, Layout\n",
    "from IPython.display import display, clear_output, Audio, HTML\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = [12.0, 8.0]\n",
    "mpl.rcParams['font.size'] = 12\n",
    "mpl.rcParams['legend.fontsize'] = 'large'\n",
    "mpl.rcParams['figure.titlesize'] = 'large'\n",
    "\n",
    "dir = \"demo/\"\n",
    "demo_files = [\"friendly.wav\",\"male1.wav\",\"female1.wav\",\"female2.wav\",\"train.wav\",\"timit_m1_sa1.wav\",\"timit_f1_sa1.wav\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def time_domain_plot(file_name,length_ms,shift_ms):\n",
    "    wavdata, sr = Spch.load_data(dir+file_name)\n",
    "    shift = shift_ms/1000.\n",
    "    length = length_ms/1000.\n",
    "    rms,pitch,zcr = Sps.time_dom3(y=wavdata,sr=sr, shift=shift, length=length)\n",
    "    rms = 10.*np.log10(rms)\n",
    "    spg = Sps.spectrogram(wavdata,sample_rate=sr,f_shift=shift,f_length=length,n_mels=None,mode='dB')\n",
    "    fig = Spd.PlotSpgFtrs(wavdata=wavdata,spgdata=spg,shift=shift,line_ftrs=[rms,pitch],dy=None,sample_rate=sr,\n",
    "                         row_heights=[1,1,1,1])\n",
    "    fig.axes[2].set_ylabel(\"ENERGY (dB)\")\n",
    "    fig.axes[3].set_ylabel(\"Pitch (Hz)\")\n",
    "    pitch_lim = np.array(fig.axes[3].get_ylim())\n",
    "    pitch_lim[0] = 50*(pitch_lim[0]//50)\n",
    "    pitch_lim[1] = 50*((pitch_lim[1]-1)//50 + 1)\n",
    "    fig.axes[3].set_ylim(pitch_lim)\n",
    "    #fig.add_seg_plot(seg,iax=0,ypos=0.8)\n",
    "    #if ZCR:\n",
    "    #    fig.add_line_plot(wavdata,dx=1/sr,iax=4)\n",
    "    #    fig.axes[4].set_ylabel(\"ZCR (/sec)\")\n",
    "    #fig.add_line_plot(4*rms,dx=shift,iax=0)\n",
    "    clear_output(wait=True)\n",
    "    display(fig)\n",
    "    display(Audio(wavdata,rate=sr))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions and Things to explore\n",
    "\n",
    "In this notebook you can explore practical issues in a sliding window analysis; this time for the extraction of two of the most essential features: energy and pitch.\n",
    "You can berify that *good* length and shift values lead to estimates that are in sync with human perception:   \n",
    "- tracking with TOO LARGE a window (large frame lengths) will smear out the time varying properties, and we may be observing global long term properties instead, instead of shorter term variations that are not audible\n",
    "- tracking with TOO SMALL a window (short frame lengths/shifts) may yield unstable estimates that suggests short term variabilities that are perceptionally irrelevant\n",
    "\n",
    "\n",
    "##### 1. Frame Length \\>\\= Frame Shift ?   \n",
    "Read the CAVEAT section first. As mentioned there, it is advised NOT to use length < shift as the software may not be reliable in such situation.   \n",
    "Which of the explanations below is correct ?  Make a sketch that supports your answer:   \n",
    "- it is technically impossible to implement a sliding window approach where length < shift \n",
    "- it makes no sense to have a frame length that is shorter than a frame shift, as this implies gaps in the observation of the signal   \n",
    "  \n",
    "##### 2. Effect of Window Length\n",
    "Setup 1: What happens when using long window lengths ?\n",
    "- Set the frame shift to 5.0 msec\n",
    "- **INCREASE** the frame length gradually from 30.0 to 200.0 msec\n",
    "    - What changes do you observe in the energy and pitch curves\n",
    "    - Do you observe phenomena / values that seem inappropriate for speech analysis ?\n",
    "    \n",
    "Setup 2: What with short window lengths and shifts ?\n",
    "- Set the frame shift to 2. (or 1.) msec\n",
    "- **DECREASE** the frame length gradually from 30.0 to 2.0 msec\n",
    "    - At some point the pitch estimate gets lost .. What is happening ?\n",
    "    - Explore both a male example and a female example\n",
    "    - Can you relate the frame length where the pitch estimate fails to the actual pitch of the voice?  Hint: the algorithm used here relies on a time difference function\n",
    "    - Come up with a rule of thumb for minimum window length to be able to do pitch estimation\n",
    "    \n",
    "Setup 3: \n",
    "- Use the same setup as in (2) and focus now on the energy\n",
    "- The very short frame shift lets you observe fine temporal details in the energy curve\n",
    "- Again let the window length slider move from high to (very) low:\n",
    "    - During a single sound we expect features such as energy to be rather stable; however, for which values of the window length see you a ripple through of the pitch onto the energy ?  Explain this with a sketch.\n",
    "\n",
    "Global Questions:\n",
    "- on the basis of the previous experiments, what can you say about reasonable values for window length and window shift ?\n",
    "\n",
    "##### CAVEATS\n",
    "- The GUI is intentionally very flexible in what values of length and shift you enter.\n",
    "- The output is not guaranteed to make sense (and you may even have occasional crashes) when you enter extreme or unnatural values (e.g. shift > length).  If that is the case, just move your sliders slightly to somewhat more *normal* values and everything will be fine again (hopefully)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Time-Domain GUI\n",
    "\n",
    "#### Interaction\n",
    "You can choose a **file** from a small selection   \n",
    "You can interact with 2 sliders: **frame_length** and **frame_shift** (in msec). Defaults are frame_length=20msec and frame_shift=10msec\n",
    "\n",
    "#### Display\n",
    "In the figure you get following views:\n",
    "- time waveform  (with energy overlay)\n",
    "- Fourier Spectrogram\n",
    "- RMS energy (square root of mean energy per sample)\n",
    "- pitch (in Hz)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c6786f04a40412da8429e9e42209266",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='File Name', options=('friendly.wav', 'male1.wav', 'female1.wav', 'â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "interact(time_domain_plot,\n",
    "    file_name = widgets.Dropdown(options=demo_files,value=demo_files[0],description=\"File Name\"),\n",
    "    length_ms=widgets.FloatSlider(value=30.,min=2.,max=200.,step=2.,\n",
    "                    continous_update=False,description=\"Frame Length (msec)\",\n",
    "                    layout=Layout(width='50%') ,style={'description_width':'50%'}),\n",
    "    shift_ms=widgets.FloatSlider(value=10.,min=2.,max=80.,step=2.,\n",
    "                    description=\"Frame Shift (msec)\",continuous_update=False,\n",
    "                    layout=Layout(width='50%') ,style={'description_width':'50%'}));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
